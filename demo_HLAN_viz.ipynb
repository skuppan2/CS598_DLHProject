{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLqbPZPoRPfU"
   },
   "source": [
    "# Colab-only Setup\n",
    "\n",
    "See `CS598DL4H/Setup.ipynb` for details of the mounted drive set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TP7HN8_lRaae",
    "outputId": "f3a2037e-dc5e-41f2-aeaf-a04e2717a8c7"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kI7kQpMFRjN3",
    "outputId": "3ffdb0fd-25bc-44b1-8739-5a1e0b149a39"
   },
   "outputs": [],
   "source": [
    "# %cd /content/gdrive/My Drive/Illinois/DL4Healthcare/Project/CS598DL4H/Explainable-Automated-Medical-Coding/\n",
    "\n",
    "# # Per https://colab.research.google.com/notebooks/tensorflow_version.ipynb, we shouldn't install tensorflow ourselves, here\n",
    "# # Instead, we should just select the pre-installed version\n",
    "# %tensorflow_version 1.x\n",
    "\n",
    "# # Moreover, it looks like the only thing needed that's not pre-installed, is tflearn, so just try that\n",
    "# ! pip install tflearn==0.5.0\n",
    "\n",
    "# %cd HLAN/\n",
    "# ! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93r6cgOHTEAB",
    "outputId": "4a496944-73a6-4510-908f-24eb8b07c818"
   },
   "outputs": [],
   "source": [
    "# # Bail out if not running on GPU\n",
    "# import tensorflow as tf\n",
    "# print(tf.__version__)\n",
    "# device_name = tf.test.gpu_device_name()\n",
    "# if device_name != '/device:GPU:0':\n",
    "#   raise SystemError('GPU device not found')\n",
    "# print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgbwmFvuN3eK"
   },
   "source": [
    "# Explainable Automated Medical Coding\n",
    "An demo of using Hierarchical Label-wise Attention Network (HLAN) for explainable medical coding.\n",
    "\n",
    "The demo allows to input a discharge summary and then predict the ICD-9 codes related to it. \n",
    "* [0. Preparation](#0.-Preparation): import libraries\n",
    "* [1. Configuration](#1.-Configuration): setting hyper-parameters\n",
    "* [2. Data preprocessing](#2.-Data-preprocessing): (i) vocabulary building; (ii) sentence parsing, tokenisation\n",
    "* [3. Prediction and visualistion](#3.-Prediction-and-visualisation): (i) loading a pre-trained HLAN model (from the MIMIC-III dataset) to predict the top 50 ICD-9 codes; (ii) visualising the word-level and sentence-level attention scores for each assigned ICD-9 code.\n",
    "\n",
    "The program does not need GPU. On a CPU, it takes around 4s (seconds) to load the model to predict, and only a further 10s to visualise a batch (by default, 32) of discharge summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_brJl6IN3eS"
   },
   "source": [
    "### 0. Preparation\n",
    "Import libraries to use for preprocessing data, loading a deep learning model, prediction, and visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4egE-B7N3e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "import warnings\n",
    "#do not show warning messages while importing tensorflow and functions that use numpy\n",
    "with warnings.catch_warnings():  \n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    import tensorflow as tf\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)  # or any {DEBUG, INFO, WARN, ERROR, FATAL}\n",
    "    #tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # or any {DEBUG, INFO, WARN, ERROR, FATAL}\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\",category=RuntimeWarning)\n",
    "    from data_util_gensim import load_data_multilabel_pre_split_for_pred,create_vocabulary,create_vocabulary_label_for_predict,get_label_sim_matrix,get_label_sub_matrix\n",
    "    from model_predict_util import preprocessing,viz_attention_scores,retrieve_icd_descs,output_to_file,display_for_qualitative_evaluation,display_for_qualitative_evaluation_per_label\n",
    "\n",
    "import time\n",
    "import pickle    \n",
    "import pandas as pd\n",
    "\n",
    "from HAN_model_dynamic import HAN    \n",
    "from tflearn.data_utils import pad_sequences\n",
    "from gensim.models import Word2Vec            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5oLWSBGN3e8"
   },
   "source": [
    "### 1. Configuration\n",
    "The setting and hyper-parameters for the HLAN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byxgn6KQN3e9"
   },
   "outputs": [],
   "source": [
    "#two key settings\n",
    "# (i) whether to input a document (otherwise, using a .txt file of documents, where each document occupies one line)\n",
    "to_input = False\n",
    "\n",
    "\n",
    "# HLAN+LE trained on MIMIC-III-50\n",
    "ckpt_dir=\"../checkpoints/checkpoint_HLAN+LE_50/\"\n",
    "dataset = \"mimic3-ds-50\"\n",
    "batch_size = 32\n",
    "per_label_attention=True\n",
    "per_label_sent_only=False\n",
    "sent_split=False\n",
    "\n",
    "# #HAGRU+LE trained on MIMIC-III-50\n",
    "# ckpt_dir=\"../checkpoints/checkpoint_HAGRU+LE_50/\"\n",
    "# dataset = \"mimic3-ds-50\"\n",
    "# batch_size = 32\n",
    "# per_label_attention=True\n",
    "# per_label_sent_only=True\n",
    "# sent_split=False\n",
    "\n",
    "# # HAN+LE trained on MIMIC-III-50\n",
    "# ckpt_dir=\"../checkpoints/checkpoint_HAN+LE_50/\"\n",
    "# dataset = \"mimic3-ds-50\"\n",
    "# batch_size = 32\n",
    "# per_label_attention=False\n",
    "# per_label_sent_only=False\n",
    "# sent_split=False\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "# (ii) the model checkpoint folder to load from - choose one from the options below\n",
    "#HLAN+LE+sent split trained on MIMIC-III-50\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN_50_per_label_bs32_sent_split_LE/\";dataset = \"mimic3-ds-50\";batch_size = 32;per_label_attention=True;per_label_sent_only=False;sent_split=False #HLAN trained on MIMIC-III-50\n",
    "\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HLAN+LE_50/\";dataset = \"mimic3-ds-50\";batch_size = 32;per_label_attention=True;per_label_sent_only=False;sent_split=False #HLAN trained on MIMIC-III-50\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN+LE_50/\";dataset = \"mimic3-ds-50\";batch_size = 128;per_label_attention=True;per_label_sent_only=False;sent_split=False #HAN trained on MIMIC-III-50\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAGRU+LE_50/\";dataset = \"mimic3-ds-50\";batch_size = 32;per_label_attention=False;per_label_sent_only=False;sent_split=False #HAGRU trained on MIMIC-III-50\n",
    "\n",
    "#HLAN+LE trained on MIMIC-III-50\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN_50_per_label_bs32_LE/\";dataset = \"mimic3-ds-50\";batch_size = 32;per_label_attention=True;per_label_sent_only=False;sent_split=True #HLAN trained on MIMIC-III-50\n",
    "\n",
    "#HLAN+LE+sent split trained on MIMIC-III-shielding\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN_shielding_th50_per_label_bs32_sent_split_LE/\";dataset = \"mimic3-ds-shielding-th50\";batch_size = 32;per_label_attention=True;per_label_sent_only=False;sent_split=True #HLAN trained on MIMIC-III-shielding\n",
    "\n",
    "#HLAN+LE trained on MIMIC-III-shielding\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN_shielding_th50_per_label_bs32_LE/\";dataset = \"mimic3-ds-shielding-th50\";batch_size = 32;per_label_attention=True;per_label_sent_only=False;sent_split=False #HLAN trained on MIMIC-III-shielding\n",
    "\n",
    "#HA-GRU trained on MIMIC-III-50\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN_50_per_label_sent_only_bs32_LE/\";dataset = \"mimic3-ds-50\";batch_size = 32;per_label_attention=True;per_label_sent_only=True;sent_split=False #HLAN trained on MIMIC-III-shielding\n",
    "\n",
    "#HA-GRU trained on MIMIC-III-shielding\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN_shielding_th50_per_label_sent_only_bs32_LE/\";dataset = \"mimic3-ds-shielding-th50\";batch_size = 32;per_label_attention=True;per_label_sent_only=True;sent_split=False #HLAN trained on MIMIC-III-shielding\n",
    "\n",
    "#HAN+sent split trained on MIMIC-III\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN_sent_split_LE/\";dataset = \"mimic3-ds\";batch_size = 128;per_label_attention=False;per_label_sent_only=False;sent_split=False #HAN trained on MIMIC-III\n",
    "\n",
    "#HAN trained on MIMIC-III\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN_LE/\";dataset = \"mimic3-ds\";batch_size = 128;per_label_attention=False;per_label_sent_only=False;sent_split=True\n",
    "\n",
    "#HAN+sent split trained on MIMIC-III-50\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN_50_LE/\";dataset = \"mimic3-ds-50\";batch_size = 128;per_label_attention=False;per_label_sent_only=False;sent_split=False\n",
    "\n",
    "#HAN+sent split trained on MIMIC-III-shielding\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN_shielding_th50_LE/\";dataset = \"mimic3-ds-shielding-th50\";batch_size = 128;per_label_attention=False;per_label_sent_only=False;sent_split=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZYFwp_KN3e-"
   },
   "outputs": [],
   "source": [
    "#other settings and hyper-parameters\n",
    "word2vec_model_path = \"../embeddings/processed_full.w2v\"\n",
    "emb_model_path = \"../embeddings/word-mimic3-ds-label.model\" #using the one learned from the full label sets of mimic-iii discharge summaries\n",
    "label_embedding_model_path = \"../embeddings/code-emb-mimic3-tr-400.model\" # for label embedding initialisation (W_projection)\n",
    "label_embedding_model_path_per_label = \"../embeddings/code-emb-mimic3-tr-200.model\" # for label embedding initialisation (per_label context_vectors)\n",
    "kb_icd9 = \"../knowledge_bases/kb-icd-sub.csv\"\n",
    "\n",
    "gpu=True\n",
    "learning_rate = 0.01\n",
    "decay_steps = 6000\n",
    "decay_rate = 1.0\n",
    "sequence_length = 2500\n",
    "num_sentences = 100\n",
    "embed_size=100\n",
    "hidden_size=100\n",
    "is_training=False\n",
    "lambda_sim=0.0\n",
    "lambda_sub=0.0\n",
    "dynamic_sem=True\n",
    "dynamic_sem_l2=False\n",
    "multi_label_flag=True\n",
    "pred_threshold=0.5\n",
    "use_random_sampling=False\n",
    "miu_factor=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d25Zl1nMN3fC"
   },
   "source": [
    "### 2. Data preprocessing\n",
    "Part1: Loading label vocabulary and building word vocabulary from pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hrFv1NxKN3fD",
    "outputId": "a9b39126-527f-4744-846c-de6c3ba08aad",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#using gpu or not\n",
    "if not gpu: \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"  \n",
    "    \n",
    "#load the label list\n",
    "vocabulary_word2index_label,vocabulary_index2word_label = create_vocabulary_label_for_predict(name_scope=dataset + \"-HAN\") # keep a distinct name scope for each model and each dataset.\n",
    "if vocabulary_word2index_label == None:\n",
    "    print('_label_vocabulary.pik file unavailable')\n",
    "    sys.exit()\n",
    "\n",
    "#get the number of labels\n",
    "num_classes=len(vocabulary_word2index_label)\n",
    "\n",
    "#building the vocabulary list from the pre-trained word embeddings\n",
    "vocabulary_word2index, vocabulary_index2word = create_vocabulary(word2vec_model_path,name_scope=dataset + \"-HAN\")\n",
    "vocab_size = len(vocabulary_word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RApZay-EN3fF"
   },
   "source": [
    "Part2: Input a discharge summary and preprocess the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aM748LvhN3fF",
    "outputId": "a7723f9c-3fb0-4187-ee85-11f137442d37",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#input or use files and preprocess a discharge summary\n",
    "if to_input:\n",
    "    print('Please input a discharge summary.');time.sleep(0.1) #sleep for proper display\n",
    "    report_raw = input(); preprocess = True\n",
    "\n",
    "    filename_to_predict = 'raw dis sum input.txt'\n",
    "    output_to_file(\"../datasets/%s\" % filename_to_predict,report_raw)\n",
    "else:\n",
    "    # directly load from file (each preprocessed document in a row)\n",
    "    if not sent_split:\n",
    "        #the sample document in the MIMIC-III-50\n",
    "        #filename_to_predict = 'sample MIMIC-III-50 test doc-24.txt'\n",
    "        #the 1730 testing documents for MIMIC-III-50\n",
    "        filename_to_predict= 'mimiciii_test_50_th0.txt'\n",
    "    else:   \n",
    "        #the sample sentence split document\n",
    "        #filename_to_predict='sample MIMIC-III-50 test doc-24 sent split.txt'\n",
    "        #the 1730 testing documents, all sentence-splitted, for MIMIC-III-50\n",
    "        filename_to_predict= 'mimiciii_test_50_sent_split_th0_for_HAN.txt'\n",
    "    preprocess = False # no further preprocessing if loading preprocessed documents\n",
    "    \n",
    "testing_data_path = \"../datasets/%s\" % filename_to_predict\n",
    "\n",
    "if preprocess:\n",
    "    #preprocess data (sentence parsing and tokenisation)\n",
    "    #this is only used for a *raw* discharge summary (one each time) and when to_input is set as true.\n",
    "    clinical_note_preprocessed_str = preprocessing(raw_clinical_note_file=testing_data_path,sent_parsing=sent_split,num_of_sen=100,num_of_sen_len=25) # tokenisation, padding, lower casing, sentence splitting\n",
    "    output_to_file('clinical_note_temp.txt',clinical_note_preprocessed_str) #load the preprocessed data\n",
    "    testX, testY = load_data_multilabel_pre_split_for_pred(vocabulary_word2index,vocabulary_word2index_label,data_path='clinical_note_temp.txt')\n",
    "else:\n",
    "    #this allows processing many preprocessed documents together, each in a row of the file in the testing_data_path\n",
    "    testX, testY = load_data_multilabel_pre_split_for_pred(vocabulary_word2index,vocabulary_word2index_label,data_path=testing_data_path)\n",
    "\n",
    "#padding to the maximum sequence length\n",
    "testX = pad_sequences(testX, maxlen=sequence_length, value=0.)  # padding to max length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-CSkmmxN3fs"
   },
   "source": [
    "### 3. Prediction and visualisation\n",
    "3.1 Load HLAN model to predict ICD-9 code\n",
    "\n",
    "<img src=\"https://github.com/dmcguire81/Explainable-Automated-Medical-Coding/blob/master/HLAN/HLAN-architecture.PNG?raw=1\" width=\"600\" height=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LvGT5G7zN3ft",
    "outputId": "5e0e4e5f-b4f2-4a8d-8a68-e44ea8e56fd4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "#create session.\n",
    "config=tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=False\n",
    "with tf.Session(config=config) as sess:\n",
    "    #Instantiate Model\n",
    "    model=HAN(num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,num_sentences,vocab_size,embed_size,hidden_size,is_training,lambda_sim,lambda_sub,dynamic_sem,dynamic_sem_l2,per_label_attention,per_label_sent_only,multi_label_flag=multi_label_flag)\n",
    "    saver=tf.train.Saver(max_to_keep = 1) # only keep the latest model, here is the best model\n",
    "    if os.path.exists(ckpt_dir+\"checkpoint\"):\n",
    "        print(\"Restoring Variables from Checkpoint\")\n",
    "        saver.restore(sess,tf.train.latest_checkpoint(ckpt_dir))\n",
    "    else:\n",
    "        print(\"Can't find the checkpoint.going to stop\")\n",
    "        sys.exit()\n",
    "\n",
    "    #get prediction results and attention scores\n",
    "    if per_label_attention: # to do for per_label_sent_only\n",
    "        prediction_str = display_for_qualitative_evaluation_per_label(sess,model,testX,testY,batch_size,vocabulary_index2word,vocabulary_index2word_label,sequence_length,per_label_sent_only,num_sentences=num_sentences,threshold=pred_threshold,use_random_sampling=use_random_sampling,miu_factor=miu_factor) \n",
    "    else:\n",
    "        prediction_str = display_for_qualitative_evaluation(sess,model,testX,testY,batch_size,vocabulary_index2word,vocabulary_index2word_label,sequence_length=sequence_length,num_sentences=num_sentences,threshold=pred_threshold,use_random_sampling=use_random_sampling,miu_factor=miu_factor)\n",
    "\n",
    "#prediction_str #to display raw attention score outputs with predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7pcMNgzN3fu"
   },
   "source": [
    "3.2 Display and save the label-wise attention visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rgZOCmu3N3fw",
    "outputId": "cb7195a5-eba5-4501-b56f-7a5fb5d33d65"
   },
   "outputs": [],
   "source": [
    "#get attention score and labels for visualisation\n",
    "list_doc_label_marks,list_doc_att_viz,dict_doc_pred = viz_attention_scores(prediction_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "ZzHAhVSdN3fw",
    "outputId": "ef300c86-96b3-47c2-df77-11bd7c6fd887"
   },
   "outputs": [],
   "source": [
    "if len(list_doc_att_viz) == 0: # if no ICD code assigned for the document.\n",
    "    print('No ICD code predicted for this document.')    \n",
    "else:    \n",
    "    for ind, (doc_label_mark, doc_att_viz) in enumerate(zip(list_doc_label_marks,list_doc_att_viz)):\n",
    "        # retrieve and display ICD-9 codes and descriptions \n",
    "        doc_label_mark_ele_list = doc_label_mark.split('-')\n",
    "        if len(doc_label_mark_ele_list)==2: # HAN model, same visualisation for all codes\n",
    "            doc_label_mark_without_code = '-'.join(doc_label_mark_ele_list[:3])\n",
    "            print(doc_label_mark_without_code)\n",
    "            filename = 'att-%s.xlsx' % (doc_label_mark[:len(doc_label_mark)-1])     \n",
    "            predictions = dict_doc_pred[doc_label_mark_without_code]\n",
    "            predictions = predictions.split('labels:')[0]\n",
    "            ICD_9_codes = predictions.split(' ')[1:]\n",
    "            print('Predicted code list:')\n",
    "            \n",
    "            for ICD_9_code in ICD_9_codes:\n",
    "                ICD_9_code = ICD_9_code.strip()\n",
    "                # retrieve the short title and the long title of this ICD-9 code\n",
    "                _, long_tit,code_type = retrieve_icd_descs(ICD_9_code)\n",
    "                print(code_type,'code:',ICD_9_code,'(',long_tit,')')\n",
    "        else: # HLAN or HA-GRU, a different visualisation for each label\n",
    "            ICD_9_code = doc_label_mark_ele_list[3] # retrieve the predicted ICD-9 code\n",
    "            ICD_9_code = ICD_9_code[:len(ICD_9_code)-1] # drop the trailing colon\n",
    "            short_tit, long_tit,code_type = retrieve_icd_descs(ICD_9_code) # retrieve the short title and the long title of this ICD-9 code\n",
    "            doc_label_mark_without_code = '-'.join(doc_label_mark_ele_list[:3])\n",
    "            print(doc_label_mark_without_code,'to predict %s code' % code_type,ICD_9_code,'(%s)' % (long_tit))\n",
    "            filename = 'att-%s(%s).xlsx' % (doc_label_mark[:len(doc_label_mark)-1],short_tit) #do not include the colon in the last char\n",
    "            filename = filename.replace('/','').replace('<','').replace('>','') # avoid slash / or <, > signs in the filename\n",
    "        \n",
    "       # COMMENT this visualization code to speed up the execution to get f1-score\n",
    "    \n",
    "       # export the visualisation to an Excel sheet\n",
    "        filename = os.sep.join(['..', 'explanations', filename]) # put the files under the ../explanations/ folder.\n",
    "        doc_att_viz.set_properties(**{'font-size': '9pt'})\\\n",
    "                   .to_excel(filename, engine='openpyxl')\n",
    "        print('Visualisation below saved to %s.' % filename) \n",
    "        \n",
    "        # reset the font for the display below\n",
    "        doc_att_viz.set_properties(**{'font-size': '5pt'})\n",
    "        display(doc_att_viz)\n",
    "        \n",
    "        #display the prediction when the label-wise visualisations for the document end\n",
    "        if ind!=len(list_doc_label_marks)-1:\n",
    "            #this is not the last doc label mark\n",
    "            if list_doc_label_marks[ind+1][:len(doc_label_mark_without_code)] != doc_label_mark_without_code:                \n",
    "                #the next doc label mark is not the current one\n",
    "                print(dict_doc_pred[doc_label_mark_without_code])\n",
    "                #print('Visualisation for %s ended.\\n' % doc_label_mark_without_code)\n",
    "        else:\n",
    "            #this is the last doc label mark\n",
    "            print(dict_doc_pred[doc_label_mark_without_code])                                   \n",
    "            #print('Visualisation for %s ended.\\n' % doc_label_mark_without_code)\n",
    "\n",
    "print(\"--- The prediction and visualisation took %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the performance\n",
    "expected_icd_codes = []\n",
    "predicted_icd_codes = {}\n",
    "   \n",
    "for doc_label_mark in list_doc_label_marks:\n",
    "    # retrieve ICD-9 codes\n",
    "    doc_label_mark_ele_list = doc_label_mark.split('-')\n",
    "    if len(doc_label_mark_ele_list)==2: # HAN model, same visualisation for all codes\n",
    "        doc_label_mark_without_code = '-'.join(doc_label_mark_ele_list[:3])\n",
    "        doc_name = doc_label_mark[:len(doc_label_mark)-1]\n",
    "        record_index = doc_name.split('-')[1]\n",
    "#         print(record_index)\n",
    "        \n",
    "        predictions = dict_doc_pred[doc_label_mark_without_code]\n",
    "        predictions = predictions.split('labels:')[0]\n",
    "        ICD_9_codes = predictions.split(' ')[1:]\n",
    "#         print('Predicted code list:')\n",
    "\n",
    "        for ICD_9_code in ICD_9_codes:\n",
    "            ICD_9_code = ICD_9_code.strip()\n",
    "            \n",
    "            key = int(record_index)\n",
    "            if key not in predicted_icd_codes:\n",
    "                predicted_icd_codes[key] = []\n",
    "            predicted_icd_codes[key].append(ICD_9_code)\n",
    "    else: # HLAN or HA-GRU, a different visualisation for each label\n",
    "        ICD_9_code = doc_label_mark_ele_list[3] # retrieve the predicted ICD-9 code\n",
    "        ICD_9_code = ICD_9_code[:len(ICD_9_code)-1] # drop the trailing colon\n",
    "        \n",
    "#         record_index = doc_label_mark[:len(doc_label_mark)-1]\n",
    "        record_index = doc_label_mark_ele_list[1]\n",
    "        \n",
    "        key = int(record_index)\n",
    "        if key not in predicted_icd_codes:\n",
    "            predicted_icd_codes[key] = []\n",
    "        predicted_icd_codes[key].append(ICD_9_code)\n",
    "        \n",
    "print('predicted_icd_codes')\n",
    "print(predicted_icd_codes)\n",
    "\n",
    "\n",
    "# Getting Test Data\n",
    "file = open(testing_data_path, 'r')\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    code_delimiter_index = line.rfind(\"__label__\")\n",
    "    expected_icd_codes.append(line[code_delimiter_index + len(\"__label__\"):].split())\n",
    "file.close()\n",
    "\n",
    "print('expected_icd_codes')\n",
    "print(expected_icd_codes)\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "# F1-Score = 2 * (precision * recall)/(precision + recall) \n",
    "for record_index in predicted_icd_codes.keys():\n",
    "    expected_icd_codes_list = expected_icd_codes[record_index]\n",
    "    predicted_codes = predicted_icd_codes[record_index]\n",
    "\n",
    "    true_positives = len(set(expected_icd_codes_list).intersection(predicted_codes))\n",
    "    precision = (true_positives/len(predicted_codes)) if len(predicted_codes) > 0 else 0\n",
    "    recall = (true_positives/len(expected_icd_codes_list)) if len(expected_icd_codes_list) > 0 else 0\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Final metrics\n",
    "precision = sum(precisions) / len(precisions)\n",
    "recall = sum(recalls) / len(recalls)\n",
    "f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "demo_HLAN_viz.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env_explainable",
   "language": "python",
   "name": "env_explainable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
